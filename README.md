# Airline Analysis Using Hadoop, Hive, Spark

My team and I completed the project when we participated in the Samsung Innovation Campus.

# Describe my project
- Hadoop: Transfer files from your local system to HDFS. This process is essential for preparing data for processing in a Hadoop environment
- Hive: We created two empty databases: one for storing raw data and another for storing data after performing the ETL process.
- Spark: We have 5 files wroten in python.
 + airports_spark.py, plane_spark.py, carriers_spark.py, detailed_data.py: load, transform, and save airport data within a Hive database using PySpark.
 + flights_fact_spark.py: This file probably deals with fact tables related to flights. It might be used for creating or processing a fact table in a data warehouse environment, particularly focusing on flight-related metrics or records.
   
# Operating System we use
 - Linux


